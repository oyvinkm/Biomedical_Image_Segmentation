{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Image_Functions import slicing,  crop_to_size\n",
    "from datasetModule import Set\n",
    "from torch import nn\n",
    "from Dice_Loss import DiceLoss\n",
    "import Model_Aske\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most of the functionallity is stored in module files. \n",
    "The data consist of Images with 3 channels and segmentation images with 2 channels.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#hyper parameters\n",
    "batch_size = 2\n",
    "learning_rate = 0.01\n",
    "num_epochs = 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\"Need to specify the local path on computer\"\n",
    "dir_path = \"../Cropped_Task3/\"\n",
    "\n",
    "'Splitting the data into 30% test and 70% training.'\n",
    "train_set, test_set = train_test_split(Set(dir_path, sub_dir = 'crop_sub-23'), test_size=0.3, random_state=25)\n",
    "\n",
    "#X_train, X_test = crop_images_to_brain(X_train), crop_images_to_brain(X_test)\n",
    "size = (256,288,176)\n",
    "train_set = crop_to_size(train_set, size)\n",
    "test_set = crop_to_size(test_set, size)\n",
    "\n",
    "'Load training and test set, batch size my vary'\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "test_set = None\n",
    "train_set = None"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../Cropped_Task3/crop_sub-233\n",
      "(275, 350, 168)\n",
      "segmentation shape:  torch.Size([1, 275, 350, 168])\n",
      "../Cropped_Task3/crop_sub-232\n",
      "(281, 330, 172)\n",
      "segmentation shape:  torch.Size([1, 281, 330, 172])\n",
      "../Cropped_Task3/crop_sub-234\n",
      "(281, 349, 172)\n",
      "segmentation shape:  torch.Size([1, 281, 349, 172])\n",
      "../Cropped_Task3/crop_sub-230\n",
      "(281, 358, 182)\n",
      "segmentation shape:  torch.Size([1, 281, 358, 182])\n",
      "../Cropped_Task3/crop_sub-231\n",
      "(322, 340, 185)\n",
      "segmentation shape:  torch.Size([1, 322, 340, 185])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "'Run the CNN'\n",
    "model = Model_Aske.CNN(3,base_features=16).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "n_total_steps = len(train_loader)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "examples = iter(train_loader)\n",
    "samples = examples.next()\n",
    "\n",
    "try:\n",
    "    while (samples['seg'][0][0].max() == 0) or (samples['seg'][0][0].max() == 0):\n",
    "        samples = examples.next()\n",
    "except StopIteration:\n",
    "    print('Iteration is shit')\n",
    "data = samples['data']\n",
    "seg = samples['seg']\n",
    "\n",
    "print(seg.max())\n",
    "print(seg.shape)\n",
    "print(data.shape)\n",
    "out = model(data)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1., dtype=torch.float64)\n",
      "torch.Size([1, 1, 256, 288, 176])\n",
      "torch.Size([1, 3, 256, 288, 176])\n",
      "Final =  torch.Size([1, 1, 256, 288, 176])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\n",
    "loss_func_1 = nn.BCELoss()\n",
    "loss_func_2 = DiceLoss()\n",
    "#target = torch.squeeze(seg, 0).type(torch.LongTensor)\n",
    "#target = seg.resize_(1,256,288,176).type(torch.LongTensor)\n",
    "seg = seg.float()\n",
    "sigmoid = nn.Sigmoid()\n",
    "out_sig = sigmoid(out)\n",
    "print(out_sig.max())\n",
    "loss_1 = loss_func_1(out_sig, seg)\n",
    "print(loss_1)\n",
    "loss_2 = loss_func_2(out, seg)\n",
    "print(loss_2) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 1, 256, 288, 176])\n",
      "tensor(27864.0117, grad_fn=<MaxBackward1>)\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.7631, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "torch.Size([1, 1, 256, 288, 176])\n",
      "tensor(1., grad_fn=<MaxBackward1>)\n",
      "tensor(1.0000, grad_fn=<RsubBackward1>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notes on how things are going: \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "BCELoss requires no squeezing, where as Cross Entropy requires the feature dimension to be squeezed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, image_set in enumerate(train_loader):\n",
    "        image = image_set['data'].to(device)\n",
    "        labels = image_set['seg'].to(device)\n",
    "        outputs = model(image)\n",
    "\n",
    "        print(\"outputs shape = \", outputs.shape)\n",
    "        print(\"squeeze = \", torch.squeeze(labels,1).shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        print(1)\n",
    "        optimizer.zero_grad()\n",
    "        print(2)\n",
    "        loss.backward()\n",
    "        print(3)\n",
    "        optimizer.step()\n",
    "        print(4)\n",
    "        if (i+1) % 1 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(out_img[0][0].shape)\n",
    "img = out_img[0][0]\n",
    "img = img.detach().numpy()\n",
    "for imgur in image[0]:\n",
    "    slicing(imgur)\n",
    "#slicing(img)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "09df6ad7c0cd894ef37e668561b864e929260a726a998a2694588e4bb4310250"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}