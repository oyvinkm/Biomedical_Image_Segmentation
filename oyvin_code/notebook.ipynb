{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Image_Functions import slicing, crop_images_to_brain, crop_to_size\n",
    "from datasetModule import Set"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Most of the functionallity is stored in module files. \n",
    "The data consist of Images with 3 channels and segmentation images with 2 channels.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\"Need to specify the local path on computer\"\n",
    "dir_path = \"../Cropped_Task3/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "'Splitting the data into 30% test and 70% training.'\n",
    "X_train, X_test = train_test_split(Set(dir_path, sub_dir = 'crop_sub-23'), test_size=0.3, random_state=25)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "../Cropped_Task3/crop_sub-233\n",
      "torch.Size([275, 350, 168])\n",
      "Shape:  torch.Size([1, 275, 350, 168])\n",
      "Type:  <class 'torch.Tensor'>\n",
      "../Cropped_Task3/crop_sub-232\n",
      "torch.Size([281, 330, 172])\n",
      "Shape:  torch.Size([1, 281, 330, 172])\n",
      "Type:  <class 'torch.Tensor'>\n",
      "../Cropped_Task3/crop_sub-234\n",
      "torch.Size([281, 349, 172])\n",
      "Shape:  torch.Size([1, 281, 349, 172])\n",
      "Type:  <class 'torch.Tensor'>\n",
      "../Cropped_Task3/crop_sub-230\n",
      "torch.Size([281, 358, 182])\n",
      "Shape:  torch.Size([1, 281, 358, 182])\n",
      "Type:  <class 'torch.Tensor'>\n",
      "../Cropped_Task3/crop_sub-231\n",
      "torch.Size([322, 340, 185])\n",
      "Shape:  torch.Size([1, 322, 340, 185])\n",
      "Type:  <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#X_train, X_test = crop_images_to_brain(X_train), crop_images_to_brain(X_test)\n",
    "size = (256,288,176)\n",
    "#size = (128,128,128)\n",
    "X_train, X_test = crop_to_size(X_train, size), crop_to_size(X_test,size)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'Visualising slices of images along all three axis'\n",
    "for i in range(len(X_train)):\n",
    "    for j in range(3):\n",
    "        img = X_train[i]['data'][j]\n",
    "        img = img.float()\n",
    "        slicing(img)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to access the set after they have been parsed through the dataloader:\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To access a batch: batch = next(iter(<<Insert name here>>))\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To access the data: batch['batchnumber']['data']\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To access the segmentation: batch['batchnumber']['seg]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'Load training and test set, batch size may vary'\n",
    "train_set, test_set = DataLoader(X_train, batch_size=1), DataLoader(X_test, batch_size=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tmp = next(iter(train_set))\n",
    "image = tmp['data']\n",
    "print(image.shape)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import parameter\n",
    "import Model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'Run the CNN'\n",
    "model = Model.CNN(3, None)\n",
    "print(model)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "out_img = model(image)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "print(out_img[0][0].shape)\n",
    "img = out_img[0][0]\n",
    "img = img.detach().numpy()\n",
    "slicing(img)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "09df6ad7c0cd894ef37e668561b864e929260a726a998a2694588e4bb4310250"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}